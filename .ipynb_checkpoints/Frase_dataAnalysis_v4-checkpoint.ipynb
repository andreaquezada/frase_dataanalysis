{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b8ab40-b5eb-495d-bdfe-4b46f8669a47",
   "metadata": {},
   "source": [
    "# **FRASE assay - Data analysis**\n",
    "\n",
    "The Fluorescence resonance energy transfer (FRET)-based mHTT aggregate seeding assay is a biophysical assay use to follow the aggregation of Huntingtin Exon 1 (HttEx1) during a time course. Besides of using this assay to study HttEx1 aggregation kinetics, it can also be used to assess the effect of inhibitors or the seeding activity of biological Htt samples.\n",
    "\n",
    "It requires the two fluorescently-label proteins (GST-Ex1Q48-CyPet or GST-Ex1Q48-YPet) as decribed in:\n",
    "\n",
    "[A. Ast, A. Buntru, F. Schindler, R. Hasenkopf, A.\n",
    "Schulz, L. Brusendorf, K. Klockmeier, G. Grelle, B.\n",
    "McMahon, H. Niederlechner, I. Jansen, L. Diez, J. Edel,\n",
    "A. Boeddrich, S. A. Franklin, B. Baldo, S. Schnoegl,\n",
    "S. Kunz, B. Purfurst, A. Gaertner, H. H. Kampinga,\n",
    "A. J. Morton,  A. Petersen, J. Kirstein, G. P. Bates, and\n",
    "E. E. Wanker. mhtt seeding activity: A marker of disease progression\n",
    "and neurotoxicity in models of huntington’s disease. Molecular Cell, 71:675–\n",
    "688.e6, 9 2018](https://www.sciencedirect.com/science/article/pii/S1097276518306014?via%3Dihub).\n",
    "\n",
    "This script is designed to analyze FRASE data from this system using a 384-well reading plate and a Tecan reader.\n",
    "\n",
    "It takes two files in .csv format:\n",
    "\n",
    "1) The pippeting scheme, containing the scheme of the 384-well plate.\n",
    "\n",
    "2) The raw data exported form the tecan in a .csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3102385d-dafb-4a8b-a980-d73ff3272a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import csv\n",
    "from scipy.optimize import curve_fit\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\" \n",
    "The pipetting scheme and the output sheet need to be exported as csv (comma separated values) files. Change the name of the .csv input file accordingly\n",
    "\"\"\"\n",
    "# Check if running in Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    colab = True\n",
    "except ImportError:\n",
    "    colab = False\n",
    "\n",
    "if colab:\n",
    "    # Colab file upload\n",
    "    print(\"Please upload the CSV file for the pipetting scheme.\")\n",
    "    uploaded_pip_file = files.upload()\n",
    "    pip_file = list(uploaded_pip_file.keys())[0]\n",
    "\n",
    "    print(\"Please upload the CSV file for the data.\")\n",
    "    uploaded_input_file = files.upload()\n",
    "    input_file = list(uploaded_input_file.keys())[0]\n",
    "else:\n",
    "    # Local file input\n",
    "    pip_file = input(\"Enter the path to the pipetting scheme CSV file: \").strip()\n",
    "    input_file = input(\"Enter the path to the data CSV file: \").strip()\n",
    "\n",
    "# Read the files\n",
    "pipetting_scheme = pd.read_csv(pip_file)\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Pipetting scheme loaded from {pip_file}.\")\n",
    "print(f\"Data loaded from {input_file}.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a dictionary that contains information about the contents of each well. This will enable the script to recognize and track the substances you placed in each well during your experiment.\n",
    "\"\"\"\n",
    "\n",
    "# Create a dictionary with the pippeting scheme. This command read the CSV file skipping the first row\n",
    "with open(pip_file, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    data = list(reader)\n",
    "\n",
    "# Initialize the dictionary to store field positions\n",
    "my_exp = {}\n",
    "\n",
    "# Iterate through the rows and columns to populate the dictionary\n",
    "for row_index, row in enumerate(data):\n",
    "    for col_index, field in enumerate(row[1:], start=1):  # Start from 1 to skip the first column\n",
    "        row_label = chr(ord('A') + row_index)  # Convert row index to letter\n",
    "        position = f\"{row_label}{col_index}\"\n",
    "\n",
    "        # Check if the field is not empty and not a repeat\n",
    "        if field and field not in my_exp:\n",
    "            # If it's a new field, add it to the dictionary with an empty list\n",
    "            my_exp[field] = []\n",
    "\n",
    "        # If the field is not empty, add the position to its list\n",
    "        if field:\n",
    "            my_exp[field].append(position)\n",
    "\n",
    "# Create a new dictionary excluding specified keys - those that are not fret experiments\n",
    "exclude_keys = ['Background', 'CyPet-only', 'YPet-only', 'Blank']\n",
    "my_exp_filtered = {key: value for key, value in my_exp.items() if key not in exclude_keys}\n",
    "\n",
    "\"\"\"\n",
    "Read the file and store the CyPet, yPet and FRET tables in separate csv files\n",
    "\"\"\"\n",
    "\n",
    "# This awk oneliner creates three separated csv files for CyPet, YPet and FRET\n",
    "awk_command = fr'''awk -F, '/^CyPet/ {{ f = \"CyPet.csv\"; }} /^YPet/ {{ f = \"YPet.csv\"; }} /^FRET/ {{ f = \"FRET.csv\"; }} f {{ print > f; }}' {input_file}'''\n",
    "result = subprocess.check_output(awk_command, shell=True, text=True)\n",
    "\n",
    "\"\"\"\n",
    "Read the tables from the corresponding csv files and store them in pandas dataframes\n",
    "\"\"\"\n",
    "\n",
    "variables = ['CyPet', 'YPet', 'FRET']\n",
    "\n",
    "# Create an empty dictionary to store the raw dataframes\n",
    "raw_df = {}\n",
    "\n",
    "for i in variables:\n",
    "    # Read the file\n",
    "    df = pd.read_csv(f'{i}.csv', skiprows=1)\n",
    "    # Transpose the DataFrame\n",
    "    df = df.transpose()\n",
    "    # Reset the header to the first row and remove the original header\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    # Drop columns with NaN (null) header\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "    # Delete Endzeit or End Time column in FRET. \n",
    "    if 'Endzeit:' in df.columns:\n",
    "        df.drop('Endzeit:', axis=1, inplace=True)\n",
    "    elif 'End Time:' in df.columns:\n",
    "        df.drop('End Time:', axis=1, inplace=True)\n",
    "\n",
    "    # Discard rows with null values\n",
    "    df = df.dropna(axis=0)\n",
    "    # Convert all the values in the dataframe to numerical values in case there are some unnoticed strings\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    # Save the DataFrame as a separate CSV file in a clean format (in case is needed)\n",
    "    #df.to_csv(f'{i}_clean.csv', index=False)\n",
    "    # Save the dataframe in the dictionary for later use\n",
    "    raw_df[i] = df\n",
    "\n",
    "# Concatenate all DataFrames in the dictionary\n",
    "raw_df = pd.concat(raw_df.values(), axis=1, keys=raw_df.keys())\n",
    "raw_df.to_csv('raw.csv', index=False)\n",
    "#print(raw_df)\n",
    "\n",
    "\"\"\"\n",
    "Substract the background\n",
    "\"\"\"\n",
    "\n",
    "# Creates an empty dictionary to store the substracted dataframes\n",
    "substracted_df = {}\n",
    "\n",
    "# Extract the well corresponding to the background. This command will take only the first well (element 0) of the Background. Change it to 1 if you want to use the duplicate instead. \n",
    "bg = my_exp['Background'][0]\n",
    "\n",
    "variables = ['CyPet', 'YPet', 'FRET']\n",
    "\n",
    "for i in variables:\n",
    "    \n",
    "    # pull the dataframe from the dictionary\n",
    "    my_df = raw_df[f'{i}']\n",
    "    \n",
    "    # List the columns of each data frame. \n",
    "    columns = list(my_df.columns.values)\n",
    "    \n",
    "    # Create a sublist of the list excluding the columns that correspond to time and Temperature.\n",
    "    data_col = columns[2:]\n",
    "    \n",
    "    # Create a list to store the new dataframes\n",
    "    substracted_dfs = []\n",
    "    \n",
    "    # Substract the bg from each column.\n",
    "    for j in data_col:\n",
    "        resta = raw_df[i][j] - raw_df[i][bg]\n",
    "        substracted_dfs.append(pd.DataFrame({f'{j}': resta}))\n",
    "    \n",
    "    # Concatenate all the DataFrames along the columns (axis=1)\n",
    "    substracted_df[f'{i}'] = pd.concat(substracted_dfs, axis=1)\n",
    "\n",
    "substracted_df['CyPet'].to_csv('subs_cypet.csv', index=False)\n",
    "substracted_df['YPet'].to_csv('subs_ypet.csv', index=False)\n",
    "substracted_df['FRET'].to_csv('subs_fret.csv', index=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Determine correction factor for CyPet and YPet\n",
    "\"\"\"\n",
    "\n",
    "# Extract the wells corresponding to CyPet and YPet only in my_exp. Assumes there is only one of them per experiment.\n",
    "cypet_only = my_exp['CyPet-only'][0]\n",
    "ypet_only = my_exp['YPet-only'][0]\n",
    "\n",
    "# Gets the correction factor for each fluorophore. \n",
    "cypet_corrfac = np.mean(substracted_df['FRET'][cypet_only]/substracted_df['CyPet'][cypet_only])\n",
    "ypet_corrfac = np.mean(substracted_df['FRET'][ypet_only]/substracted_df['YPet'][ypet_only])\n",
    "\n",
    "#print(cypet_corrfac)\n",
    "#print(ypet_corrfac)\n",
    "\n",
    "\"\"\"\n",
    "Obtaining False FRET\n",
    "\"\"\"\n",
    "\n",
    "# Create an empty dictionary to store the substracted dataframes excluding the background, the cypet-only and the ypet-only wells\n",
    "two_fluo_df = {}\n",
    "\n",
    "# Define the columns to exclude in the new dataframes\n",
    "columns_to_exclude = my_exp['Background'] + my_exp['CyPet-only'] + my_exp['YPet-only']\n",
    "#print(columns_to_exclude)\n",
    "\n",
    "variables = ['CyPet', 'YPet', 'FRET']\n",
    "\n",
    "for l in variables:\n",
    "    two_fluo_df[f'{l}'] = substracted_df[f'{l}'].drop(columns=columns_to_exclude)\n",
    "#print(two_fluo_df['FRET'])\n",
    "\n",
    "# Get False Frate\n",
    "false_fret = two_fluo_df['CyPet']*cypet_corrfac + two_fluo_df['YPet']*ypet_corrfac\n",
    "\n",
    "#print(false_fret)\n",
    "false_fret.to_csv('false.csv', index=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Obtaining Real FRET\n",
    "\"\"\"\n",
    "\n",
    "# Define the data in my_exp that contains actual FRET experiments - with both fluorophores present\n",
    "real_fret = two_fluo_df['FRET'] - false_fret\n",
    "\n",
    "#print(real_fret)\n",
    "real_fret.to_csv('real.csv', index=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Obtaining Clean Normalized FRET. Discard \"bad\" data. Too many negative values and infinite values.\n",
    "\"\"\"\n",
    "\n",
    "# Define the data in my_exp that contains actual FRET experiments - with both fluorophores present\n",
    "norm_fret = real_fret/two_fluo_df['YPet']*100\n",
    "\n",
    "#print(norm_fret)\n",
    "norm_fret.to_csv('norm.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Obtaining Clean Normalized FRET. Discard \"bad\" data. More than 20 negative values and infinite values.\n",
    "\"\"\"\n",
    "\n",
    "# Calculate normalized FRET\n",
    "norm_fret = real_fret / two_fluo_df['YPet'] * 100\n",
    "\n",
    "# Identify and discard columns with more than 20 negative values or any infinite values\n",
    "columns_to_discard = []\n",
    "negative_threshold = 20\n",
    "\n",
    "for column in norm_fret.columns:\n",
    "    negative_count = (norm_fret[column] < 0).sum()  # Count of negative values in the column\n",
    "    has_infinite = np.isinf(norm_fret[column]).any()  # Check for infinite values\n",
    "\n",
    "    # Discard columns with more than 20 negative values or any infinite values\n",
    "    if negative_count > negative_threshold or has_infinite:\n",
    "        columns_to_discard.append(column)\n",
    "\n",
    "# Warn the user about discarded columns\n",
    "if columns_to_discard:\n",
    "    warnings.warn(f\"Columns {', '.join(columns_to_discard)} were discarded because they contain more than {negative_threshold} negative values or infinite values.\")\n",
    "\n",
    "# Drop columns with more than 20 negative values or any infinite values\n",
    "clean_norm_fret = norm_fret.drop(columns=columns_to_discard)\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV file\n",
    "clean_norm_fret.to_csv('clean_norm_fret.csv', index=False)\n",
    "\n",
    "# Optional: print or return the cleaned DataFrame if needed\n",
    "#print(clean_norm_fret)\n",
    "\n",
    "# Check if any of the discarded headers appear as values in `my_exp_filtered`\n",
    "headers_in_values = []\n",
    "for key, values in my_exp_filtered.items():\n",
    "    for header in columns_to_discard:\n",
    "        if header in values:\n",
    "            headers_in_values.append(key)\n",
    "            break  # No need to check further for this key\n",
    "            \n",
    "# Remove entries from `my_exp_filtered` where the discarded headers are found as values\n",
    "cleaned_exp_filtered = {k: v for k, v in my_exp_filtered.items() if k not in headers_in_values}\n",
    "\n",
    "# Warn the user about removed dictionary entries\n",
    "if headers_in_values:\n",
    "    warnings.warn(f\"Entries {', '.join(headers_in_values)} were discarded from `my_exp_filtered` because they contained headers from invalid columns as values.\")\n",
    "\n",
    "# Print the final cleaned dictionary\n",
    "# print(cleaned_exp_filtered)\n",
    "\n",
    "\"\"\"\n",
    "Separate triplicates. Get the mean and the standard deviation. Fit.\n",
    "\"\"\"\n",
    "\n",
    "# Identify the correct time column.\n",
    "time_column = None\n",
    "if ('CyPet',   'Zeit [s]') in raw_df.columns:\n",
    "    time_column = ('CyPet',   'Zeit [s]')\n",
    "elif ('CyPet',   'Time [s]') in raw_df.columns:\n",
    "    time_column = ('CyPet',   'Time [s]')\n",
    "\n",
    "# Obtain time values in hours. \n",
    "time = raw_df[time_column] / 3600\n",
    "\n",
    "# Create a DataFrame for the 'time' column\n",
    "time_df = pd.DataFrame({'time': time})\n",
    "\n",
    "# Define the generalized Hill equation\n",
    "def hill_equation(x, EC50, Top, Bottom, HillSlope, S):\n",
    "    Denominator = (1 + (2 ** (1/S) - 1) * ((EC50/x) ** HillSlope)) ** S\n",
    "    Numerator = Top - Bottom\n",
    "    return Bottom + (Numerator / Denominator)\n",
    "\n",
    "# Create a dictionary to store the fitted curves\n",
    "fitted_curves_dict = {}\n",
    "\n",
    "# Creates an empty dataframe to store the data to plot\n",
    "data_list = []\n",
    "\n",
    "# Define the name of each FRASE experiment that was made by triplicates\n",
    "exp_fret = list(cleaned_exp_filtered)\n",
    "\n",
    "# Create a new DataFrame for storing t50 values\n",
    "t50_df = pd.DataFrame(columns=['Key', 't50_rep1', 't50_rep2', 't50_rep3', 't50_mean', 't50_sd'])\n",
    "\n",
    "# Get the average of each triplicate and the standard deviation. \n",
    "for m in exp_fret:\n",
    "\n",
    "    storem = [] # Empty list to store the data for each experiment\n",
    "    col_to_include = my_exp[f'{m}']\n",
    "    \n",
    "    # Get the data for each triplicate\n",
    "    for j, k in enumerate(col_to_include):\n",
    "        j = j + 1\n",
    "        storem.append(pd.DataFrame({f'{m}_rep{j}':clean_norm_fret[k]}))\n",
    "        \n",
    "        # set x and y values\n",
    "        x = time\n",
    "        y = clean_norm_fret[k]\n",
    "        \n",
    "        # Initial parameter guesses (you might need to adjust these based on your data)\n",
    "        initial_params = [50, 100, 0, 1, 2]\n",
    "        \n",
    "        # Fit the data to the hill equation\n",
    "        params, _ = curve_fit(hill_equation, x, y, p0=initial_params)\n",
    "\n",
    "        # Extract the t50 parameter\n",
    "        t50_value = params[0]\n",
    "    \n",
    "        # Check if the Key already exists in the DataFrame\n",
    "        if m in t50_df['Key'].values:\n",
    "            # Update the existing row with the new t50 value\n",
    "            t50_df.loc[t50_df['Key'] == m, f't50_rep{j}'] = t50_value\n",
    "        else:\n",
    "            # Create a new row with the Key and t50 value\n",
    "            new_row = pd.DataFrame({'Key': [m], f't50_rep{j}': [t50_value]})\n",
    "            t50_df = pd.concat([t50_df, new_row], ignore_index=True)\n",
    "\n",
    "    # Calculate the mean and standard deviation for each row\n",
    "    t50_df['t50_mean'] = t50_df[['t50_rep1', 't50_rep2', 't50_rep3']].mean(axis=1)\n",
    "    t50_df['t50_sd'] = t50_df[['t50_rep1', 't50_rep2', 't50_rep3']].std(axis=1)\n",
    "\n",
    "\n",
    "    # Get the mean for each exp\n",
    "    mean_data = clean_norm_fret[col_to_include].mean(axis=1)\n",
    "    data_list.append(pd.DataFrame({f'{m}':mean_data}))\n",
    "    storem.append(pd.DataFrame({f'{m}_mean':mean_data}))\n",
    "    \n",
    "    # Get the standard deviation\n",
    "    sd_data = clean_norm_fret[col_to_include].std(axis=1)\n",
    "    data_list.append(pd.DataFrame({f'std_{m}':sd_data}))\n",
    "    storem.append(pd.DataFrame({f'{m}_sd':sd_data}))\n",
    "    \n",
    "    # Replace conflicting character /\n",
    "    m = m.replace('/', '_')\n",
    "    \n",
    "    # Concatenate the storem list containing information of each triplicate and the time column into a dataframe and save it as csv\n",
    "    df = pd.concat(storem, axis=1)\n",
    "    df = pd.concat([df, time_df], axis=1)\n",
    "    df.to_csv(f'data_{m}.csv', index=False)\n",
    "     \n",
    "    \n",
    "# Concatenate all the means and sd in a dataframe named frase_plot\n",
    "frase_plot = pd.concat(data_list, axis=1)\n",
    "\n",
    "# Concatenate time_df with frase_plot\n",
    "frase_plot = pd.concat([frase_plot, time_df], axis=1)\n",
    "frase_plot.to_csv('fraseplot.csv', index=False)\n",
    "\n",
    "# Print the resulting t50 DataFrame and fitted curves\n",
    "t50_df.to_csv('t50_replicates.csv', index=False)\n",
    "print(tabulate(t50_df, headers='keys', tablefmt='psql'))\n",
    "#print(fitted_curves_dict)\n",
    "\n",
    "\"\"\"\n",
    "Fit the curves, obtain t50 and plot the data\n",
    "\"\"\"\n",
    "\n",
    "# Define the name of each FRASE experiment that was made by triplicates\n",
    "exp_fret = list(cleaned_exp_filtered)\n",
    "\n",
    "\n",
    "# Define the generalized Hill equation\n",
    "def hill_equation(x, EC50, Top, Bottom, HillSlope, S):\n",
    "    Denominator = (1 + (2 ** (1/S) - 1) * ((EC50/x) ** HillSlope)) ** S\n",
    "    Numerator = Top - Bottom\n",
    "    return Bottom + (Numerator / Denominator)\n",
    "\n",
    "# Create a new DataFrame for storing t50 values\n",
    "t50_mean = pd.DataFrame()\n",
    "\n",
    "# Create a dictionary to store the fitted curves\n",
    "fitted_curves_dict = {}\n",
    "\n",
    "# Read time and fret for each mean curve and assign variables x and y\n",
    "for l in exp_fret:\n",
    "    x = frase_plot['time']  \n",
    "    y = frase_plot[l]  \n",
    "\n",
    "    # Initial parameter guesses (you might need to adjust these based on your data)\n",
    "    initial_params = [50, 100, 0, 1, 2]\n",
    "\n",
    "    # Fit the data to the hill equation\n",
    "    params, _ = curve_fit(hill_equation, x, y, p0=initial_params)\n",
    "\n",
    "    # Extract the t50 parameter\n",
    "    t50_value = params[0]\n",
    "    \n",
    "    # Append the key and t50 value to the new DataFrame\n",
    "    t50_mean = pd.concat([t50_mean, pd.DataFrame({'Key': [l], 't50_mean': [t50_value]})], ignore_index=True)\n",
    "\n",
    "    # Generate the fitted curve using the fitted parameters\n",
    "    fitted_curve = hill_equation(x, *params)\n",
    "\n",
    "    # Store the fitted curve in the dictionary\n",
    "    fitted_curves_dict[l] = fitted_curve\n",
    "\n",
    "# Print the resulting t50 DataFrame and fitted curves\n",
    "t50_mean.to_csv('t50_mean.csv', index=False)\n",
    "print(tabulate(t50_mean, headers='keys', tablefmt='psql'))\n",
    "print(type(t50_mean))\n",
    "\n",
    "\"\"\"\n",
    "Plot the data\n",
    "\"\"\"\n",
    "# Define the colormap\n",
    "\n",
    "# Define a custom palette with MDC colors Blue, Dark Blue, Dark Red, Dark Teal, Light Blue, Light Red, Light Teal, Red, Teal \n",
    "custom_palette = ['#1e3791', '#141955', '#780050', '#005055', '#87aadc', '#faafaf', '#9bd7d2', '#eb2d4b', '#00ac8c']\n",
    "#custom_palette = ['#1e3791', '#00ac8c']\n",
    "\n",
    "custom_palette = [\n",
    "    '#f3e5f5',  # Light Purple\n",
    "    '#e1bee7',\n",
    "    '#ce93d8',\n",
    "    '#ba68c8',\n",
    "    '#ab47bc',\n",
    "    '#9c27b0',\n",
    "    '#8e24aa',\n",
    "    '#7b1fa2',\n",
    "    '#6a1b9a',\n",
    "    '#4a148c',\n",
    "    '#9c27b0',\n",
    "    '#ab47bc',\n",
    "    '#ba68c8',\n",
    "    '#ce93d8',\n",
    "    '#e1bee7',\n",
    "    '#f3e5f5',\n",
    "    '#f8bbd0',\n",
    "    '#f48fb1',\n",
    "    '#f06292',\n",
    "    '#ec407a'\n",
    "]\n",
    "\n",
    "\n",
    "# Check if custom_palette is defined\n",
    "use_custom_palette = 'custom_palette' in locals()\n",
    "\n",
    "# ... or used one from the system\n",
    "#palette = cm.viridis\n",
    "#palette = cm.gist_stern\n",
    "#palette = cm.brg\n",
    "#palette = cm.jet\n",
    "#palette = cm.hot\n",
    "\n",
    "\n",
    "#for column in filtered_exp:\n",
    "for i, column in enumerate(exp_fret):\n",
    "    if use_custom_palette:\n",
    "        color = custom_palette[i % len(custom_palette)]  # Cycle through custom colors\n",
    "    else:\n",
    "        color = cm.viridis(i / len(exp_fret))  # Use viridis colormap\n",
    "    \n",
    "    plt.errorbar(frase_plot['time'], frase_plot[f'{column}'], yerr=frase_plot[f'std_{column}'], label = column, capsize=3, fmt='.', linestyle='None', color=color)\n",
    "    plt.plot(frase_plot['time'], fitted_curves_dict[f'{column}'], color=color)\n",
    "\n",
    "frase_plot.to_csv('plot_values.csv', index=False)\n",
    "fit_pd = pd.DataFrame(fitted_curves_dict)\n",
    "fit_pd.to_csv('fit.csv', index=False)\n",
    "\n",
    "plt.xlabel('Time (h)', fontweight='bold', size=12)\n",
    "plt.ylabel('FRET efficiency (%)', fontweight='bold', size=12)\n",
    "plt.yticks(fontweight='bold', size=10)\n",
    "plt.xticks(fontweight='bold', size=10)\n",
    "plt.legend(loc='center', bbox_to_anchor=(1.25, 0.5))\n",
    "\n",
    "plt.savefig('frase.svg')\n",
    "plt.savefig('frase.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b414598a-d90f-4685-aa66-9b2ffb6479fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
